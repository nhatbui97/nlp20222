{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-28T17:59:05.256676Z","iopub.execute_input":"2023-06-28T17:59:05.257902Z","iopub.status.idle":"2023-06-28T17:59:05.304886Z","shell.execute_reply.started":"2023-06-28T17:59:05.257864Z","shell.execute_reply":"2023-06-28T17:59:05.303498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:59:05.307644Z","iopub.execute_input":"2023-06-28T17:59:05.308139Z","iopub.status.idle":"2023-06-28T17:59:07.713208Z","shell.execute_reply.started":"2023-06-28T17:59:05.308099Z","shell.execute_reply":"2023-06-28T17:59:07.711972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/amazondata/Train.csv')\ndf_val = pd.read_csv('/kaggle/input/amazondata/Valid.csv')\ndf_test = pd.read_csv('/kaggle/input/amazondata/Test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-06-28T17:59:07.715190Z","iopub.execute_input":"2023-06-28T17:59:07.715675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def take_subset(ds_train, ds_val, ds_test, percentage):\n    return ds_train[:int(ds_train.shape[0] * percentage)], ds_val[:int(ds_val.shape[0] * percentage)], ds_test[:int(ds_test.shape[0] * percentage)]\ndf_train, df_val, df_test = take_subset(df_train,df_val,df_test,0.03)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Score_list = [] ; word_count_list =[]\nfor Score in df['Score'].unique():\n    df_filter = df.loc[(df['Score'] == Score)]\n    word_count_temp = df_filter['Text'].str.split().str.len().sum()\n    Score_list.append(Score)\n    word_count_list.append(word_count_temp)\nword_count_df = pd.DataFrame({'Score':Score_list, 'Word Count':word_count_list})\nword_count_df['Word Count'] = word_count_df['Word Count'].astype('int')\nword_count_df = word_count_df.sort_values('Word Count', ascending=False)\nword_count_df.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_word_count = df['Text'].str.split().str.len().sum()\nprint(f'The word count of all text is: {int(total_word_count)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vc(df, column, r=False):\n    vc_df = df.reset_index().groupby([column]).size().to_frame('count')\n    vc_df['percentage (%)'] = vc_df['count'].div(sum(vc_df['count'])).mul(100)\n    vc_df = vc_df.sort_values(by=['percentage (%)'], ascending=False)\n    if r:\n        return vc_df\n    else:\n        print(f'STATUS: Value counts of \"{column}\"...')\n        display(vc_df)\nvc(df, 'Score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to print data shape\nprint(f'data shape is: {df.shape}')\n\n# to identify the null values by descending order\ndf.isnull().sum().sort_values(ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lower(df, attribute):\n    df.loc[:,attribute] = df[attribute].apply(lambda x : str.lower(x))\n    return df\ndf = lower(df,'Text')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To remove transcription punctuation and numbers\n\nwarnings.filterwarnings('ignore')\ndef remove_punc_num(df, attribute):\n    df.loc[:,attribute] = df[attribute].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n    df[attribute] = df[attribute].str.replace('\\d+', '')\n    return df\ndf =remove_punc_num(df, 'Text')\ndf_no_punc =df.copy()\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to tokenise transcription\n\n# import nltk\ntk =WhitespaceTokenizer()\ndef tokenise(df, attribute):\n    df['tokenised'] = df.apply(lambda row: tk.tokenize(str(row[attribute])), axis=1)\n    return df\ndf =tokenise(df, 'Text')\ndf_experiment =df.copy()\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\ndef stemming(df, attribute):\n    # Use English stemmer.\n    stemmer = SnowballStemmer(\"english\")\n    df['stemmed'] = df[attribute].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n    return df\ndf =stemming(df_experiment, 'tokenised')\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing the list of the English stop words, it has a number of 179 stop words in this list\n\nstop = stopwords.words('english')\nprint(f\"There are {len(stop)} stop words \\n\")\nprint(stop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing stop words\ndef remove_stop_words(df, attribute):\n    stop = stopwords.words('english')\n    df['stemmed_without_stop'] = df[attribute].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))\n    return df\ndf = remove_stop_words(df, 'stemmed')\ndf.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =df.drop(['Text','stemmed', 'tokenised'], axis=1)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_word_count_normalised = df['stemmed_without_stop'].str.split().str.len().sum()\nprint(f'The word count of transcription after normalised is: {int(total_word_count_normalised)}')\nprint(f'{round((total_word_count - total_word_count_normalised)/total_word_count*100, 2)}% less word')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(df['Score'])\ndf['encoded_target'] = le.transform(df['Score'])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to flatten one list\ndef flat_list(unflat_list):\n    flatted = [item for sublist in unflat_list for item in sublist]\n    return flatted\n\ndef to_list(df, attribute):\n    # Select the normalised transcript column \n    df_transcription = df[[attribute]]\n    # To convert the attribute into list format, but it has inner list. So it cannot put into the CountVectoriser\n    unflat_list_transcription = df_transcription.values.tolist()\n    # Let's use back the function defined above, \"flat_list\", to flatten the list\n    flat_list_transcription = flat_list(unflat_list_transcription)\n    return flat_list_transcription\nflat_list_transcription = to_list(df, 'stemmed_without_stop')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_gram_features ={'unigram':(1,1),'unigram_bigram':(1,2),'bigram':(2,2),\\\n       'bigram_trigram':(2,3),'trigram':(3,3)}\nfeature_name=[]\ntemp=[]\nfor key, values in n_gram_features.items():\n    temp.append(key)\n    feature_name.append(key)\ntemp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_n_gram_features(flat_list_transcription):\n    temp=[]\n    for key, values in n_gram_features.items(): \n        vectorizer = CountVectorizer(ngram_range=values)\n        vectorizer.fit(flat_list_transcription)\n        temp.append(vectorizer.transform(flat_list_transcription))\n    return temp\ntemp = generate_n_gram_features(flat_list_transcription)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframes = {'unigram':temp[0], \n              'unigram_bigram':temp[1], \n              'bigram':temp[2], \n              'bigram_trigram':temp[3], \n              'trigram':temp[4]}\nfeature_vector = [] ; feature_vector_shape = []\nfor key in dataframes:\n    feature_vector.append(key)\n    feature_vector_shape.append(dataframes[key].shape)\n\nn_gram_df = pd.DataFrame({'N-Gram Feature Vector':feature_vector, 'Data Dimension':feature_vector_shape})\nn_gram_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to retrieve a unigram feature vector\ndataframes['unigram']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport warnings\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\n\nwarnings.filterwarnings('ignore')\nrandom_state_number =8888\ndf_target =df[['encoded_target']].values.ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = {\n    'f1':[f1_score, 'f1_macro'], \n    'precision': [precision_score, 'precision_macro'], \n    'recall': [recall_score, 'recall_macro']\n}\n\n# get evaluation result\n\ndef get_performance(param_grid, base_estimator, dataframes):\n    df_name_list =[]; best_estimator_list=[]; best_score_list=[]; test_predict_result_list=[];\n    metric_list = [];\n    \n    for df_name, df in dataframes.items():\n        \n        X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.2, random_state=random_state_number)\n        for _, metric_dict in metrics.items():\n            sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5, scoring=metric_dict[1],random_state=random_state_number,\n                                      factor=2).fit(X_train, y_train)\n\n            best_estimator = sh.best_estimator_\n            clf = best_estimator.fit(X_train, y_train)\n            prediction = clf.predict(X_test)\n            test_predict_result = metric_dict[0](y_test, prediction, average='macro')\n\n            df_name_list.append(df_name) ; best_estimator_list.append(best_estimator) ; \n            best_score_list.append(sh.best_score_) ; \n            test_predict_result_list.append(test_predict_result) ;metric_list.append(metric_dict[1])\n            \n            \n    model_result = pd.DataFrame({'Vector':df_name_list,'Metric':metric_list,\n                               'Calibrated Estimator':best_estimator_list,\n                               'Best CV Metric Score':best_score_list, 'Test Predict Metric Score': test_predict_result_list})\n    return model_result\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'max_depth': [None,30,32,35,37,38,39,40],'min_samples_split': [2,150,170,180,190,200]}\nbase_estimator = RandomForestClassifier(random_state=random_state_number)\nrfc_result = get_performance(param_grid, base_estimator, dataframes)\nrfc_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"font = {'family' : 'Tahoma',\n        'weight' : 'bold',\n        'size'   : 12}\nmatplotlib.rc('font', **font)\n\ndef vis_classification(vector_type = 'unigram', estimator = KNeighborsClassifier(n_neighbors=9)):\n    pca = PCA(n_components=2)\n    df1 = pca.fit_transform(dataframes[vector_type].todense())\n    X_train, X_test, y_train, y_test = train_test_split(df1, df_target, test_size=0.2, random_state=random_state_number)\n    \n    # get training set\n    df2 = pd.DataFrame({'pca1':X_train[:,1], 'pca2': X_train[:,0], 'y':le.inverse_transform(y_train)})\n    min_1, max_1 = df2['pca1'].min(), df2['pca1'].max()\n    min_2, max_2 = df2['pca2'].min(), df2['pca2'].max()\n    \n    # generate dimension reduced, but extended data\n    pca1_range = np.linspace(min_1,max_1,30)\n    pca2_range = np.linspace(min_2,max_2,30)\n    \n    # shuffle\n    np.random.shuffle(pca1_range) ; np.random.shuffle(pca2_range)\n    \n    # to dataframe\n    prediction_test = pd.DataFrame({'pca1':pca1_range, 'pca2':pca2_range})\n\n    best_estimator = estimator\n    \n    # fit training set and predict extended data\n    clf = best_estimator.fit(X_train, y_train)\n\n    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(15,6))\n    cmap = plt.cm.get_cmap('tab10', 4)\n    fig.suptitle(f\"Visualising {type(estimator).__name__} on {vector_type.capitalize()} Vector\", fontsize=14,fontweight='bold')\n\n\n    def plot_scatter(ax, predictor_set, target, title):\n        \n        # plot area classifier\n        clf = best_estimator.fit(X_train, y_train)\n        axs[0].tricontourf(X_train[:,0], X_train[:,1], clf.predict(X_train), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n        \n        axs[1].tricontourf(X_test[:,0], X_test[:,1], clf.predict(X_test), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n        \n        # plot scatter\n        df3 = pd.DataFrame({'pca1':predictor_set[:,1], 'pca2': predictor_set[:,0], 'y':le.inverse_transform(target)})\n        for y_label in df3['y'].unique():\n            df_filter = df3[df3['y']==y_label]\n            ax.scatter(df_filter['pca1'], df_filter['pca2'], alpha=1,label=f\"{y_label}\")\n        ax.legend()\n        ax.set_title(f'{title} ({predictor_set.shape[0]} Samples)',fontweight='bold')\n    plot_scatter(axs[0], X_train, y_train, 'Training Set')\n    plot_scatter(axs[1], X_test, y_test, 'Testing Set')\n    axs[0].sharey(axs[1])\n    return plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = rfc_result[rfc_result['Metric'] =='f1_macro']\n# df_temp['Calibrated Estimator']\nvector_rfc = df_temp[['Vector','Calibrated Estimator']].set_index('Vector').to_dict()['Calibrated Estimator']\nvector_rfc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supported_columns_dict = {}\nfor df_name, df in dataframes.items():\n    X_train, X_test, y_train, y_test = train_test_split(dataframes[df_name], df_target, test_size=0.2, random_state=random_state_number)\n\n    selector = SelectFromModel(estimator=vector_rfc[df_name]).fit(X_train, y_train)\n    \n    filter_columns = selector.get_support()\n    dataframes[df_name] = dataframes[df_name][:, filter_columns]\n    \nshape_dim = [] ; df_names = []\nfor df_name, df in dataframes.items():\n    shape_dim.append(df.shape)\n    df_names.append(df_name)\nn_gram_df_dim = pd.DataFrame({'N-Gram Feature Vector':df_names, 'Data Dimension':shape_dim}) \nn_gram_df_dim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = n_gram_df_dim['N-Gram Feature Vector'].values\nb4 = [shape[1] for shape in n_gram_df['Data Dimension'].values]\naf = [shape[1] for shape in n_gram_df_dim['Data Dimension'].values]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 6))\nrects1 = ax.bar(x - width/2, b4, width, label='Before Dimensionality Reduction', color='skyblue')\nrects2 = ax.bar(x + width/2, af, width, label='After Dimensionality Reduction', color='lime')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number Columns')\nax.set_title('Before and After Dimensionality Reduction')\nax.set_xticks(x, labels)\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19,21]}\nbase_estimator = KNeighborsClassifier()\nknn_result = get_performance(param_grid, base_estimator, dataframes)\nknn_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'max_depth': [None,4,6,7,8,30,32,35],'min_samples_split': [2,3,4,5,35,10,16,20]}\nbase_estimator = DecisionTreeClassifier(random_state=random_state_number)\ndtc_result = get_performance(param_grid, base_estimator, dataframes)\ndtc_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.concat([knn_result, \n                      dtc_result,\n                      rfc_result\n                      ]\n                     ).reset_index(drop=True)\n\ndf_result.groupby(['Metric']).max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_best_result(df_result, metric_score):\n    df_result_t = df_result[df_result.Metric== 'precision_macro']\n    precision_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n\n    df_result_t = df_result[df_result.Metric== 'recall_macro']\n    recall_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n    \n    df_result_t = df_result[df_result.Metric== 'f1_macro']\n    f1_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n\n    return pd.concat([precision_macro_df,recall_macro_df,f1_macro_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_cv_result = get_best_result(df_result, 'Best CV Metric Score')\ndisplay(best_cv_result)\ntemp = best_cv_result[best_cv_result['Metric'] == 'f1_macro']\nbest_clf = temp['Calibrated Estimator'].values[0]\nbest_vector = temp['Vector'].values[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_best_result(df_result, 'Test Predict Metric Score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dataframes[best_vector], df_target, test_size=0.2, \\\n                                                    random_state=random_state_number)\nclf = best_clf.fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)\ntarget_names = ['1', '2', '3', '4', '5']\nprint(classification_report(y_test,y_test_pred,target_names=target_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_predict = pd.DataFrame({'Actual Y Test': le.inverse_transform(y_test),'Best Prediction':le.inverse_transform(y_test_pred)})\nsample_predict.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\npickle.dump(clf,open(\"bestNgrams.h5\",\"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = pickle.load(open(\"bestNgrams.h5\",\"rb\"))\ny_test_pred= clf.predict(X_test)\nsample_predict = pd.DataFrame({'Actual Y Test': le.inverse_transform(y_test),'Best Prediction':le.inverse_transform(y_test_pred)})\nsample_predict.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}